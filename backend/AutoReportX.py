import asyncio
import aiohttp
import gradio as gr
import io
import os
import re
import nest_asyncio
from tavily import TavilyClient
from together import Together
from openai import OpenAI
from gradio.components import File
from io import BytesIO

nest_asyncio.apply()

# ===============================================
# Configuration Constants
# ===============================================
from dotenv import load_dotenv

load_dotenv()  # Load bi·∫øn m√¥i tr∆∞·ªùng t·ª´ file .env

TOGETHER_API_KEY = os.getenv("TOGETHER_API_KEY")
TAVILY_API_KEY = os.getenv("TAVILY_API_KEY")
OPENAI_API_KEY = os.getenv("MY_OPENAI_API_KEY")

DEFAULT_MODEL_TOGETHER = "deepseek-ai/DeepSeek-R1-Distill-Llama-70B"
FINAL_REPORT_MODEL_OPENAI = "o1-mini"  # Supports up to ~65k tokens for completion

# L·ª±a ch·ªçn chunk size & limit
MAX_LINKS_PER_ITERATION = 30
MAX_TOKENS_INPUT = 40000  # TƒÉng/gi·∫£m t√πy √Ω, 40k an to√†n
CHUNK_SIZE = 24000        # M·ªói chunk < 24k chars
SUMMARIZE_COMPLETION_TOKENS = 2000  # Summarize chunk
FINAL_COMPLETION_TOKENS = 64000     # <= 65536 limit

together_client = Together(api_key=TOGETHER_API_KEY)
openai_client = OpenAI(api_key=OPENAI_API_KEY)
tavily_client = TavilyClient(api_key=TAVILY_API_KEY)

# ==================================
# Asynchronous Helper Functions
# ==================================

async def call_together_ai_async(messages, model=DEFAULT_MODEL_TOGETHER):
    """G·ªçi Together AI ƒë·ªÉ t·∫°o search queries."""
    try:
        response = await asyncio.to_thread(
            together_client.chat.completions.create,
            messages=messages,
            model=model,
            temperature=0.7,
            max_tokens=500
        )
        return response.choices[0].message.content.strip()
    except Exception as e:
        print("‚ùå Together AI Error:", e)
        return None

async def call_openai_async(messages, max_comp_tokens, model=FINAL_REPORT_MODEL_OPENAI):
    """
    G·ª≠i y√™u c·∫ßu ƒë·∫øn OpenAI, t√πy max_comp_tokens <= 65536.
    """
    try:
        response = await asyncio.to_thread(
            openai_client.chat.completions.create,
            model=model,
            messages=messages,
            max_completion_tokens=max_comp_tokens
        )
        return response.choices[0].message.content.strip()
    except Exception as e:
        print("‚ùå OpenAI Error:", e)
        return None

# =============== Chunk Summarization =============

async def chunk_text(text, chunk_size=CHUNK_SIZE):
    """
    Chia text th√†nh nhi·ªÅu chunk < chunk_size.
    """
    for i in range(0, len(text), chunk_size):
        yield text[i: i+chunk_size]

async def summarize_chunk(session, chunk_text):
    """
    T√≥m t·∫Øt 1 chunk, d√πng OpenAI v·ªõi SUMMARIZE_COMPLETION_TOKENS.
    """
    prompt = (
        "You are an AI summarizer. Summarize the text below into a concise overview, "
        "preserving key details but significantly reducing length.\n\nText:\n"
    )
    messages = [
        {"role": "assistant", "content": "You are a summarizer tool."},
        {"role": "user", "content": prompt + chunk_text}
    ]
    summary = await call_openai_async(messages, max_comp_tokens=SUMMARIZE_COMPLETION_TOKENS)
    return summary if summary else ""

# =============== Generate Search Queries =============
async def generate_search_queries_async(session, user_query):
    """
    D√πng Together AI ƒë·ªÉ t·∫°o query (t·ªëi ƒëa 4).
    """
    prompt = (
        "You are an expert research assistant. Given the user's query, generate up to four distinct, "
        "precise search queries that would help gather comprehensive information on the topic. "
        "Return only a Python list of strings, for example: ['query1', 'query2', 'query3']."
    )
    messages = [
        {"role": "system", "content": "You are a helpful and precise research assistant."},
        {"role": "user", "content": f"User Query: {user_query}\n\n{prompt}"}
    ]
    resp = await call_together_ai_async(messages)
    if not resp:
        print("‚ö†Ô∏è No response from Together AI, fallback queries.")
        return [
            f"{user_query} overview",
            f"{user_query} in-depth analysis",
            f"{user_query} use cases",
            f"{user_query} latest developments"
        ]
    # clean <think>...
    resp_clean = re.sub(r"<think>.*?</think>", "", resp, flags=re.DOTALL).strip()
    try:
        if resp_clean.startswith("[") and resp_clean.endswith("]"):
            sq = eval(resp_clean)
            if isinstance(sq, list) and all(isinstance(q, str) for q in sq):
                return sq
    except Exception as e:
        print("‚ö†Ô∏è Error parsing search queries:", e, "\nResponse:", resp_clean)

    # fallback
    return [
        f"{user_query} overview",
        f"{user_query} in-depth analysis",
        f"{user_query} use cases",
        f"{user_query} latest developments"
    ]

# =============== Perform Search ===============
async def perform_search_async(session, query):
    """Tavily search, l·ªçc youtube & deepseek."""
    try:
        resp = await asyncio.to_thread(
            tavily_client.search,
            query=query,
            max_results=10,
            include_raw_content=True,
            search_depth="advanced"
        )
        if not resp:
            return []
        links = [x.get("url") for x in resp.get("results", []) if "url" in x]
        valid_links = [u for u in links if "youtube.com" not in u.lower() and "deepseek.com" not in u.lower()]
        return valid_links
    except Exception as e:
        print("‚ùå Error performing Tavily search:", e)
        return []

# =============== Extract Webpage Text ===============
async def extract_webpage_text_async(url):
    """
    Tr√≠ch xu·∫•t n·ªôi dung t·ª´ m·ªôt trang web b·∫±ng Tavily.
    In ra ƒë·ªô d√†i ban ƒë·∫ßu (raw_length) tr∆∞·ªõc khi c·∫Øt c√≤n MAX_TOKENS_INPUT.
    """
    try:
        response = await asyncio.to_thread(
            tavily_client.extract,
            urls=[url],
            include_images=False
        )
        if "results" in response and response["results"]:
            raw_text = response["results"][0].get("raw_content", "")
            raw_length = len(raw_text)
            print(f"Raw length for {url}: {raw_length} chars (before cutting)")

            # C·∫Øt c√≤n MAX_TOKENS_INPUT ƒë·ªÉ tr√°nh v∆∞·ª£t context
            trimmed_text = raw_text[:MAX_TOKENS_INPUT]
            print(f"Trimmed length for {url}: {len(trimmed_text)} chars (after cutting)\n")
            return trimmed_text
        else:
            print(f"‚ö†Ô∏è No extraction result for URL: {url}")
            return ""
    except Exception as e:
        print(f"‚ùå Error extracting webpage text with Tavily for {url}: {e}")
        return ""

async def process_link(session, link, user_query, search_query):
    """L·∫•y text 1 link."""
    print(f"üîç Fetching and extracting content from: {link}")
    text = await extract_webpage_text_async(link)
    return text if text else None

# =============== Generate Final Report ===============
async def generate_final_report_async(session, user_query, all_contexts):
    """
    T·∫°o b√°o c√°o cu·ªëi c√πng, chunk Summaries => final.
    """
    if not all_contexts:
        return f"‚ö†Ô∏è Limited information found for: {user_query}."

    # G·ªôp all context
    combined = "\n".join(all_contexts)
    if len(combined) <= CHUNK_SIZE:
        # G·ªçi th·∫≥ng
        print("‚úÖ Single chunk (no chunk summarization needed).")
        long_prompt = (
            "You are an AI research assistant. Based on the following contexts and the user query, "
            "create a thoroughly detailed, long-form report. Use headings, bullet points, examples, references, etc. "
            f"Output can be up to {FINAL_COMPLETION_TOKENS} tokens if needed."
        )
        messages = [
            {"role": "assistant", "content": "You are a specialized long-form report writer."},
            {"role": "user", "content": f"User Query: {user_query}\n\nContexts:\n{combined}\n\n{long_prompt}"}
        ]
        final_report = await call_openai_async(messages, max_comp_tokens=FINAL_COMPLETION_TOKENS)
        return final_report if final_report else "‚ö†Ô∏è No significant data."

    # ...n·∫øu text > chunk size => chunk summarization
    print("‚ö†Ô∏è Context too large, using chunk summarization...")

    # Summaries
    summaries = []
    async for chunk in chunk_text(combined, CHUNK_SIZE):
        summary = await summarize_chunk(session, chunk)
        summaries.append(summary)

    # Gom summaries
    summaries_combined = "\n\n".join(summaries)
    print(f"‚úÖ Summaries combined length = {len(summaries_combined)} chars")

    # G·ªçi final
    final_prompt = (
        "You are an AI research assistant. The text below are chunk summaries. "
        "Please combine them into a single cohesive, multi-sectional, long-form report. "
        f"You may produce up to {FINAL_COMPLETION_TOKENS} tokens. Include references, headings, examples, etc."
    )
    messages = [
        {"role": "assistant", "content": "You are a specialized long-form report writer with no explicit token limit."},
        {
            "role": "user",
            "content": f"User Query: {user_query}\n\nSummaries:\n{summaries_combined}\n\n{final_prompt}"
        }
    ]
    final_report = await call_openai_async(messages, max_comp_tokens=FINAL_COMPLETION_TOKENS)
    return final_report if final_report else "‚ö†Ô∏è No final data."

# =========================
# Main Asynchronous Routine
# =========================

async def gradio_interface(user_query, iteration_limit=1):
    """H√†m ch·∫°y async_main() v√† hi·ªÉn th·ªã k·∫øt qu·∫£ tr√™n giao di·ªán Gradio."""
    aggregated_contexts = []
    all_search_queries = []
    iteration = 0

    async with aiohttp.ClientSession() as session:
        new_search_queries = await generate_search_queries_async(session, user_query)
        if not new_search_queries:
            return "‚ùå Kh√¥ng t·∫°o ƒë∆∞·ª£c truy v·∫•n t√¨m ki·∫øm."

        all_search_queries.extend(new_search_queries)

        while iteration < iteration_limit:
            search_tasks = [perform_search_async(session, query) for query in new_search_queries]
            search_results = await asyncio.gather(*search_tasks)

            unique_links = {}
            for idx, links in enumerate(search_results):
                query = new_search_queries[idx]
                for link in links:
                    if link and link not in unique_links:
                        unique_links[link] = query

            if not unique_links:
                iteration += 1
                continue

            link_tasks = [process_link(session, link, user_query, unique_links[link]) for link in unique_links]
            link_results = await asyncio.gather(*link_tasks)

            iteration_contexts = [res for res in link_results if res]
            aggregated_contexts.extend(iteration_contexts)

            if not iteration_contexts:
                iteration += 1
                continue

            new_search_queries = await generate_search_queries_async(session, user_query)
            if not new_search_queries:
                break

            all_search_queries.extend(new_search_queries)
            iteration += 1

        if not aggregated_contexts:
            return "‚ùå Kh√¥ng c√≥ n·ªôi dung ph√π h·ª£p."

        final_report = await generate_final_report_async(session, user_query, aggregated_contexts)
        return final_report

def run_gradio(user_query, iteration_limit=1):
    """H√†m ƒë·ªìng b·ªô ch·∫°y async_main() ƒë·ªÉ t√≠ch h·ª£p v√†o Gradio."""
    return asyncio.run(gradio_interface(user_query, iteration_limit))

#=============================================================================
# H√†m t·∫°o file Word
#=============================================================================

import pypandoc  # type: ignore
from io import BytesIO

def create_word_file(content, filename='report.docx'):
    file_stream = BytesIO()
    output = pypandoc.convert_text(content, 'docx', format='md', outputfile=filename)
    with open(filename, "rb") as f:
        file_stream.write(f.read())
    file_stream.seek(0)
    return file_stream, filename

#============================================================================
# H√†m t·∫°o PDF
#============================================================================

from io import BytesIO
from reportlab.lib.pagesizes import letter # type: ignore
from reportlab.pdfgen import canvas # type: ignore
from reportlab.lib.units import inch # type: ignore
from reportlab.pdfbase.pdfmetrics import stringWidth # type: ignore
from reportlab.pdfbase.ttfonts import TTFont # type: ignore
from reportlab.pdfbase import pdfmetrics # type: ignore

# ƒêƒÉng k√Ω font
font_path = r"C:\Source_Code_AutoReportX\backend\font\times.ttf"  # ƒê·∫£m b·∫£o ƒë∆∞·ªùng d·∫´n ƒë√∫ng
pdfmetrics.registerFont(TTFont("Times New Roman", font_path))


def create_pdf_file(content, filename='report.pdf'):
    try:
        # T·∫°o m·ªôt file PDF trong b·ªô nh·ªõ
        file_stream = BytesIO()
        pdf = canvas.Canvas(file_stream, pagesize=letter)

        # ƒê·∫∑t font v√† k√≠ch th∆∞·ªõc
        font_name = "Times New Roman"
        font_size = 13
        pdf.setFont(font_name, font_size)

        # Gi·ªõi h·∫°n chi·ªÅu r·ªông trang (tr·ª´ l·ªÅ)
        page_width, page_height = letter
        margin = 30  # L·ªÅ tr√°i & ph·∫£i
        max_width = page_width - 2 * margin  # Chi·ªÅu r·ªông kh·∫£ d·ª•ng
        y_position = page_height - 50  # V·ªã tr√≠ b·∫Øt ƒë·∫ßu

        # X·ª≠ l√Ω xu·ªëng d√≤ng t·ª± ƒë·ªông
        def split_text(text, max_width, font_name, font_size):
            words = text.split()
            lines = []
            current_line = ""

            for word in words:
                test_line = current_line + " " + word if current_line else word
                if stringWidth(test_line, font_name, font_size) <= max_width:
                    current_line = test_line
                else:
                    lines.append(current_line)
                    current_line = word

            if current_line:
                lines.append(current_line)

            return lines

        # X·ª≠ l√Ω t·ª´ng d√≤ng vƒÉn b·∫£n
        text_lines = []
        for line in content.split("\n"):
            text_lines.extend(split_text(line, max_width, font_name, font_size))

        # V·∫Ω t·ª´ng d√≤ng l√™n trang PDF
        line_spacing = 14  # Kho·∫£ng c√°ch d√≤ng
        for line in text_lines:
            if y_position < 50:  # N·∫øu h·∫øt trang -> t·∫°o trang m·ªõi
                pdf.showPage()
                pdf.setFont(font_name, font_size)
                y_position = page_height - 50

            pdf.drawString(margin, y_position, line)
            y_position -= line_spacing

        # K·∫øt th√∫c trang v√† l∆∞u file PDF
        pdf.showPage()
        pdf.save()

        # ƒê∆∞a con tr·ªè v·ªÅ ƒë·∫ßu file stream
        file_stream.seek(0)

        return file_stream, filename
    except Exception as e:
        print(f"L·ªói khi t·∫°o file PDF: {str(e)}")
        raise e


# ==============================================================================
# Giao di·ªán Gradio
# ==============================================================================

# H√†m t·∫£i file
def download_report(content, file_type):
    """T·∫°o file t√πy theo lo·∫°i (Word ho·∫∑c PDF) v√† tr·∫£ v·ªÅ ƒë·ªÉ t·∫£i xu·ªëng."""
    if file_type == "doc":
        return create_word_file(content)
    elif file_type == "pdf":
        return create_pdf_file(content)
    else:
        raise ValueError("Lo·∫°i file kh√¥ng h·ª£p l·ªá. Ch·ªçn 'doc' ho·∫∑c 'pdf'.")

def run_gradio(user_query, iteration_limit=1):
    """H√†m ƒë·ªìng b·ªô ch·∫°y async_main() ƒë·ªÉ t√≠ch h·ª£p v√†o Gradio."""
    final_report = asyncio.run(gradio_interface(user_query, iteration_limit))
    return final_report

def main():
    """H√†m ch√≠nh ƒë·ªÉ kh·ªüi ch·∫°y giao di·ªán Gradio."""
    with gr.Blocks() as demo:
        gr.Markdown("# üîç Open Deep Researcher - AI Research Assistant")

        with gr.Row():
            user_input = gr.Textbox(label="Nh·∫≠p ch·ªß ƒë·ªÅ nghi√™n c·ª©u", placeholder="Nh·∫≠p c√¢u h·ªèi c·ªßa b·∫°n...")
            iter_input = gr.Number(label="S·ªë v√≤ng l·∫∑p t·ªëi ƒëa", value=5)

        output_text = gr.Textbox(label="K·∫øt qu·∫£ nghi√™n c·ª©u", lines=10)

        submit_btn = gr.Button("üîé B·∫Øt ƒë·∫ßu t√¨m ki·∫øm")
        submit_btn.click(run_gradio, inputs=[user_input, iter_input], outputs=output_text)

        with gr.Row():
            download_doc_btn = gr.Button("üì• T·∫£i v·ªÅ file Word")
            download_pdf_btn = gr.Button("üì• T·∫£i v·ªÅ file PDF")

        # K·∫øt n·ªëi n√∫t t·∫£i xu·ªëng v·ªõi h√†m x·ª≠ l√Ω
        download_doc_btn.click(
            fn=download_report,
            inputs=[output_text, gr.State("doc")],  # Truy·ªÅn n·ªôi dung v√† lo·∫°i file
            outputs=gr.File(label="T·∫£i v·ªÅ file Word"),  # ƒê·∫ßu ra l√† file ƒë·ªÉ t·∫£i xu·ªëng
        )
        download_pdf_btn.click(
            fn=download_report,
            inputs=[output_text, gr.State("pdf")],  # Truy·ªÅn n·ªôi dung v√† lo·∫°i file
            outputs=gr.File(label="T·∫£i v·ªÅ file PDF"),  # ƒê·∫ßu ra l√† file ƒë·ªÉ t·∫£i xu·ªëng
        )

    demo.queue() # B·∫≠t Queue ƒë·ªÉ x·ª≠ l√Ω nhi·ªÅu y√™u c·∫ßu ƒë·ªìng th·ªùi
    demo.launch(share=True)

if __name__ == "__main__":
    main()